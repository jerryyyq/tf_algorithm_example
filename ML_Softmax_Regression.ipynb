{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- do_train: start -----------------\n",
      "file_path =  ['/home/yangyuqi/work/tf_algorithm_example/iris.csv'] \n",
      "\n",
      "Tensor(\"ReaderRead_2:0\", shape=(), dtype=string) Tensor(\"ReaderRead_2:1\", shape=(), dtype=string)\n",
      "features_0 tensor.shape = [10  4], tensor = [[ 4.69999981  3.20000005  1.60000002  0.2       ]\n",
      " [ 5.19999981  4.0999999   1.5         0.1       ]\n",
      " [ 5.5         4.19999981  1.39999998  0.2       ]\n",
      " [ 5.19999981  3.5         1.5         0.2       ]\n",
      " [ 4.80000019  3.0999999   1.60000002  0.2       ]\n",
      " [ 4.5999999   3.5999999   1.          0.2       ]\n",
      " [ 4.80000019  3.4000001   1.89999998  0.2       ]\n",
      " [ 5.          3.4000001   1.60000002  0.40000001]\n",
      " [ 5.0999999   3.5         1.39999998  0.30000001]\n",
      " [ 5.4000001   3.4000001   1.5         0.40000001]]\n",
      "\n",
      "label_0 tensor.shape = [10], tensor = [0 0 0 1 1 1 1 0 1 1]\n",
      "\n",
      "step_0 loss:  tensor.shape = [], tensor = 1.2348291873931885\n",
      "\n",
      "training end. step_9 final loss:  tensor.shape = [], tensor = 1.3672845363616943\n",
      "\n",
      "save_path is:  /tmp/ML_Softmax_Regression.vari\n",
      "At evaluate, the __W tensor.shape = [4 3], tensor = [[-0.02346934 -0.04598799  0.06945734]\n",
      " [ 0.0085389  -0.02724912  0.01871022]\n",
      " [-0.05919094 -0.03424251  0.09343345]\n",
      " [-0.02412267 -0.01430601  0.03842868]]\n",
      "\n",
      "At evaluate, the __b tensor.shape = [3], tensor = [-0.00043757 -0.00700514  0.00744271]\n",
      "\n",
      "At evaluate, test_features tensor.shape = [10  4], tensor = [[ 5.0999999   3.79999995  1.89999998  0.40000001]\n",
      " [ 5.          3.5         1.60000002  0.60000002]\n",
      " [ 5.19999981  3.4000001   1.39999998  0.2       ]\n",
      " [ 6.30000019  2.5         4.9000001   1.5       ]\n",
      " [ 5.          2.          3.5         1.        ]\n",
      " [ 5.19999981  2.70000005  3.9000001   1.39999998]\n",
      " [ 4.4000001   3.20000005  1.29999995  0.2       ]\n",
      " [ 4.9000001   2.4000001   3.29999995  1.        ]\n",
      " [ 5.0999999   3.29999995  1.70000005  0.5       ]\n",
      " [ 4.4000001   3.          1.29999995  0.2       ]]\n",
      "\n",
      "At evaluate, test_label tensor.shape = [10], tensor = [0 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "evaluate_result tensor.shape = [], tensor = 0.0\n",
      "\n",
      "----------------- do_train: finish -----------------\n",
      "-------------- do_evaluate: start -----------------\n",
      "\n",
      "file_path =  ['/home/yangyuqi/work/tf_algorithm_example/iris.csv'] \n",
      "\n",
      "Tensor(\"ReaderRead_3:0\", shape=(), dtype=string) Tensor(\"ReaderRead_3:1\", shape=(), dtype=string)\n",
      "At evaluate, the __W tensor.shape = [4 3], tensor = [[-0.02346934 -0.04598799  0.06945734]\n",
      " [ 0.0085389  -0.02724912  0.01871022]\n",
      " [-0.05919094 -0.03424251  0.09343345]\n",
      " [-0.02412267 -0.01430601  0.03842868]]\n",
      "\n",
      "At evaluate, the __b tensor.shape = [3], tensor = [-0.00043757 -0.00700514  0.00744271]\n",
      "\n",
      "At evaluate, test_features tensor.shape = [10  4], tensor = [[ 4.9000001   3.0999999   1.5         0.1       ]\n",
      " [ 5.5         4.19999981  1.39999998  0.2       ]\n",
      " [ 4.80000019  3.0999999   1.60000002  0.2       ]\n",
      " [ 5.0999999   3.5         1.39999998  0.30000001]\n",
      " [ 4.69999981  3.20000005  1.60000002  0.2       ]\n",
      " [ 5.          3.4000001   1.5         0.2       ]\n",
      " [ 5.5         3.5         1.29999995  0.2       ]\n",
      " [ 5.69999981  3.79999995  1.70000005  0.30000001]\n",
      " [ 4.9000001   3.0999999   1.5         0.1       ]\n",
      " [ 5.          3.4000001   1.60000002  0.40000001]]\n",
      "\n",
      "At evaluate, test_label tensor.shape = [10], tensor = [0 0 0 1 0 1 1 1 0 0]\n",
      "\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "FetchOutputs node Shape_324:0: not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    449\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    451\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: FetchOutputs node Shape_324:0: not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e172d1b700c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mone_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'iris.csv'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mone_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_evaluate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'iris.csv'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yangyuqi/work/tf_algorithm_example/ML_Model.py\u001b[0m in \u001b[0;36mdo_evaluate\u001b[0;34m(self, file_name, batch_size)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0maccuracy_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_echo_tensor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0maccuracy_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accuracy_rate'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yangyuqi/work/tf_algorithm_example/ML_Model.py\u001b[0m in \u001b[0;36m_echo_tensor\u001b[0;34m(self, tensor, prefix)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# 注意： print() 显示时会把元素之间的逗号去掉\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'{0} tensor.shape = {1}, tensor = {2}{3}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinesep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'{0} not_tensor = {1}{2}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinesep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: FetchOutputs node Shape_324:0: not found"
     ]
    }
   ],
   "source": [
    "######################## 机器学习对数几率回归类 ##############################\n",
    "# Author: 杨玉奇\n",
    "# email: yangyuqi@sina.com\n",
    "# url: https://github.com/jerryyyq/tf_algorithm_example\n",
    "# copyright yangyuqi\n",
    "# 著作权归作者 杨玉奇 所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n",
    "# date: 2017-09-19\n",
    "###################################################################\n",
    "\n",
    "from ML_Model import ML_Model\n",
    "import tensorflow as tf\n",
    "    \n",
    "class ML_Softmax_Regression( ML_Model ):\n",
    "    def __init__(self):\n",
    "        ML_Model.__init__(self)\n",
    "\n",
    "        self.__W = tf.Variable(tf.zeros([4, 3]), name = 'weights')\n",
    "        self.__b = tf.Variable(tf.zeros([3]), name = 'bias')\n",
    "    \n",
    "        \n",
    "    def combine_inputs(self, features):\n",
    "        return tf.matmul(features, self.__W) + self.__b\n",
    "    \n",
    "    def inference(self, features):\n",
    "        return tf.nn.softmax( self.combine_inputs(features) )\n",
    "    \n",
    "    def loss(self, features, label):\n",
    "        label_predicted = self.combine_inputs(features)\n",
    "        return tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(label_predicted, label) )\n",
    "    \n",
    "    \n",
    "    def inputs(self, file_name = [], batch_size = 10):\n",
    "        sepal_length, sepal_width, setal_length, setal_width, label = \\\n",
    "        self._read_csv(batch_size, file_name, [[0.0], [0.0], [0.0], [0.0], ['']])\n",
    "\n",
    "        # return passenger_id, survived\n",
    "        label_number = tf.to_int32(tf.argmax(tf.to_int32(tf.pack([\n",
    "            tf.equal(label, ['Iris-setosa']),\n",
    "            tf.equal(label, ['Iris-versicolor']),\n",
    "            tf.equal(label, ['Iris-virginica'])\n",
    "        ])), 0))\n",
    "        \n",
    "        features = tf.transpose(tf.pack([sepal_length, sepal_width, setal_length, setal_width]))\n",
    "\n",
    "        return features, label_number \n",
    "        \n",
    "        \n",
    "    def train(self, loss):    \n",
    "        learning_rate = 0.01\n",
    "        return tf.train.GradientDescentOptimizer( learning_rate ).minimize( loss )\n",
    "\n",
    "    \n",
    "    def evaluate(self, test_features, test_label):\n",
    "        self._echo_tensor(self.__W, 'At evaluate, the __W')\n",
    "        self._echo_tensor(self.__b, 'At evaluate, the __b')\n",
    "        self._echo_tensor(test_features, 'At evaluate, test_features')\n",
    "        self._echo_tensor(test_label, 'At evaluate, test_label')\n",
    "        \n",
    "        label_predicted = tf.cast( tf.arg_max(self.inference(test_features), 1), tf.int32 )\n",
    "        \n",
    "        return tf.reduce_mean(tf.cast(tf.equal(label_predicted, test_label), tf.float32))\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    one_ml = ML_Softmax_Regression()\n",
    "    # data from: https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "    one_ml.do_train( 10, ['iris.csv'] )\n",
    "\n",
    "    one_ml.do_evaluate( ['iris.csv'] )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 中间计算数据样例展示\n",
    "import tensorflow as tf\n",
    "\n",
    "f = [[5.5, 4.19999981, 1.39999998, 0.2],\n",
    "     [5.0999999, 3.4000001, 1.5, 0.2],\n",
    "     [5.69999981, 3.79999995, 1.70000005, 0.30000001]]\n",
    "\n",
    "label = [['Iris-setosa']\n",
    "tf.pack([\n",
    "            tf.equal(label, ['Iris-setosa']),\n",
    "            tf.equal(label, ['Iris-versicolor']),\n",
    "            tf.equal(label, ['Iris-virginica'])\n",
    "        ]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
