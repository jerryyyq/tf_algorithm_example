1.数据库太小一般不会带来不收敛的问题，只要你一直在train总会收敛（rp问题跑飞了不算）。反而不收敛一般是由于样本的信息量太大导致网络不足以fit住整个样本空间。样本少只可能带来过拟合的问题，你看下你的training set上的loss收敛了吗？如果只是validate set上不收敛那就说明overfitting了，这时候就要考虑各种anti-overfit的trick了，比如dropout，SGD，增大minibatch的数量，减少fc层的节点数量，momentum，finetune等。
2.learning rate设大了会带来跑飞（loss突然一直很大）的问题，这个是新手最常见的情况——为啥网络跑着跑着看着要收敛了结果突然飞了呢？可能性最大的原因是你用了relu作为激活函数的同时使用了softmax或者带有exp的函数做分类层的loss函数。当某一次训练传到最后一层的时候，某一节点激活过度（比如100），那么exp(100)=Inf，发生溢出，bp后所有的weight会变成NAN，然后从此之后weight就会一直保持NAN，于是loss就飞起来辣。会为了模拟这个情况，我复现了一下一年前我的一次失败的实验。我花了整个过程的loss曲线：
其中红色是loss，绿色是accuracy。可以看出在2300左右的时候跑飞了一次，不过所幸lr设的并不是非常大所以又拉了回来。如果lr设的过大会出现跑飞再也回不来的情况。这时候你停一下随便挑一个层的weights看一看，很有可能都是NAN了。对于这种情况建议用二分法尝试。0.1~0.0001.不同模型不同任务最优的lr都不一样。
3.尽量收集更多的数据。有个方法是爬flickr，找名人标签，然后稍微人工剔除一下就能收集一套不错的样本。其实收集样本不在于多而在于hard，比如你收集了40张基本姿态表情相同的同一个人的图片不如收集他的10张不同表情的图片。之前做过试验，50张variance大的图per person和300多张类似的图per person训练出来的模型后者就比前者高半个点。
4.尽量用小模型。如果数据太少尽量缩小模型复杂度。考虑减少层数或者减少kernel number。


1，        每次训练样本数目大小最少保证不要小于目标类别的2倍，否则准确率一直上不去，假如标签有40类，则不要至少小于80，越大的话对准确度越好，但训练时间越长。

2，        使用relu激励函数要比较小心，因为他在负半轴恒等为0，训练到后期，准确度突然降为0，可以考虑是不是relu的问题
如果你的softmax层输出为nan，则可考虑是不是在计算信息交叉熵时出现了log(0)

3，        全连接层节点数对结果影响较小。

4，        更换batch大小，更换激励函数，不用从头开始训练，可以与原先保存的模型文件训练


1、learning rate设大了

0.1~0.0001.不同模型不同任务最优的lr都不一样。

2、归一化



train loss 不断下降，test loss不断下降，说明网络仍在学习;

train loss 不断下降，test loss趋于不变，说明网络过拟合;

train loss 趋于不变，test loss不断下降，说明数据集100%有问题;

train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;

train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题。



1、You Forgot to Normalize Your Data
2、You Forgot to Check your Results
3、You Forgot to Preprocess Your Data
4、You Forgot to use any Regularization
5、You Used a too Large Batch Size
6、You Used an Incorrect Learning Rate
7、You Used the Wrong Activation Function on the Final Layer
8、Your Network contains Bad Gradients
9、You Initialized your Network Weights Incorrectly
10、You Used a Network that was too Deep
11、You Used the Wrong Number of Hidden Units