{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- do_train: start -----------------\n",
      "0 -W:  <tf.Variable 'weights_8:0' shape=(784, 10) dtype=float32_ref>\n",
      "WARNING:tensorflow:From /home/yangyuqi/ve_tensorflow/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "1 _W:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "1_0 cost:  Tensor(\"Print_15:0\", shape=(), dtype=float32)\n",
      "11_0 features_in:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "11_0 labels_in:  [[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "2_1 cost:  nan\n",
      "11_1 features_in:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "11_1 labels_in:  [[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "2_1 cost:  nan\n",
      "11_2 features_in:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "11_2 labels_in:  [[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "2_1 cost:  nan\n",
      "11_3 features_in:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "11_3 labels_in:  [[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "2_1 cost:  nan\n",
      "11_4 features_in:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "11_4 labels_in:  [[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "2_1 cost:  nan\n",
      "11_5 features_in:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "11_5 labels_in:  [[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "2_1 cost:  nan\n",
      "11_6 features_in:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "11_6 labels_in:  [[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "2_1 cost:  nan\n",
      "11_7 features_in:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "11_7 labels_in:  [[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "2_1 cost:  nan\n",
      "11_8 features_in:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "11_8 labels_in:  [[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "2_1 cost:  nan\n",
      "11_9 features_in:  [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "11_9 labels_in:  [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "2_1 cost:  nan\n",
      "2_0 cost:  Tensor(\"Print_15:0\", shape=(), dtype=float32)\n",
      "2 _W:  [[ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " ..., \n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]\n",
      " [ nan  nan  nan ...,  nan  nan  nan]]\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "def _read32(bytestream):\n",
    "    dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n",
    "    return int(numpy.frombuffer(bytestream.read(4), dtype=dt)[0])\n",
    "\n",
    "def _dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = numpy.arange(num_labels) * num_classes\n",
    "    labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    labels_one_hot = labels_one_hot.astype(numpy.float32)\n",
    "    return labels_one_hot\n",
    "\n",
    "def input_mnist_data(file_path):\n",
    "    with gfile.Open(file_path, 'rb') as bytestream:\n",
    "        magic = _read32(bytestream)\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Invalid magic number %d in MNIST image file: %s' % (magic, file_path))\n",
    "\n",
    "        num_images = _read32(bytestream)\n",
    "        rows = _read32(bytestream)\n",
    "        cols = _read32(bytestream)\n",
    "        buf = bytestream.read(rows * cols * num_images)\n",
    "        data = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "        data = data.reshape(num_images, rows * cols * 1)\n",
    "        data = data.astype(numpy.float32)\n",
    "        return data\n",
    "    \n",
    "def input_mnist_label(file_path):    \n",
    "    with gfile.Open(file_path, 'rb') as bytestream:\n",
    "        magic = _read32(bytestream)\n",
    "        if magic != 2049:\n",
    "            raise ValueError( 'Invalid magic number %d in MNIST label file: %s' % (magic, file_path) )\n",
    "        \n",
    "        num_items = _read32(bytestream)\n",
    "        buf = bytestream.read(num_items)\n",
    "        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "        return _dense_to_one_hot(labels, 10)\n",
    "\n",
    "\n",
    "def _get_next_data(image_file, label_file, batch_size):\n",
    "    train_data = input_mnist_data(image_file)\n",
    "    train_label = input_mnist_label(label_file)\n",
    "    step = 0\n",
    "    for step in range( len(train_data) // batch_size ):\n",
    "        begin = step * batch_size\n",
    "        end = begin + batch_size\n",
    "        if end >= len(train_data):\n",
    "            end = len(train_data) - 1\n",
    "\n",
    "        yield train_data[begin : end], train_label[begin : end]\n",
    "\n",
    "\n",
    "def inference(features):\n",
    "    combine_input = tf.matmul(features, _W) + _b\n",
    "    inference = tf.nn.softmax( combine_input )      \n",
    "    inference = tf.Print(inference, [inference, combine_input], '******* inference: ')\n",
    "    return inference\n",
    "    \n",
    "print( '-------------- do_train: start -----------------' )\n",
    "\n",
    "\n",
    "_W = tf.Variable(tf.zeros([784, 10]), name = 'weights')\n",
    "_b = tf.Variable(tf.zeros([10]), name = 'bias')    \n",
    "\n",
    "print( '0 -W: ', _W )\n",
    "       \n",
    "train_data_generator = _get_next_data( './MNIST_data/train-images.idx3-ubyte', \n",
    "                                      './MNIST_data/train-labels.idx1-ubyte', 5 )\n",
    "\n",
    "# features, labels = next( data_generator )\n",
    "# print( '0 label: ', labels )\n",
    "features = tf.placeholder(\"float\", shape=[None, 784])\n",
    "labels = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "combine_input = tf.matmul(features, _W) + _b\n",
    "combine_input = tf.Print(combine_input, [combine_input], '0 ****************** combine_input: ')\n",
    "label_predicted = tf.nn.softmax( combine_input )\n",
    "cost = -tf.reduce_mean( labels * tf.log( label_predicted ) )\n",
    "\n",
    "#label_predicted = inference( features )\n",
    "#print( '0 label_predicted: ', label_predicted )\n",
    "\n",
    "\n",
    "\n",
    "#cost = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(logits = combine_input, labels = labels) )\n",
    "cost = tf.Print(cost, [cost], '0 ****************** cost: ')\n",
    "\n",
    "with tf.control_dependencies([tf.Print(cost, [cost], '0 ****************** cost: ')]):\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize( cost )\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run( tf.initialize_all_variables() )\n",
    "\n",
    "\n",
    "print( '1 _W: ', sess.run(_W) )\n",
    "print( '1_0 cost: ', cost )\n",
    "# print( '1_1 cost: ', sess.run(cost) )\n",
    "\n",
    "#print( '1_1 label_predicted: ', sess.run(label_predicted) )\n",
    "#print( '1_1 labels_t: ', sess.run(labels_t) )\n",
    "for i in range(10):\n",
    "    features_in, labels_in = next( train_data_generator )\n",
    "    print( '11_{} features_in: '.format(i), features_in )\n",
    "    print( '11_{} labels_in: '.format(i), labels_in )\n",
    "    \n",
    "    sess.run( train_step, feed_dict={features: features_in, labels: labels_in} )\n",
    "    print( '2_1 cost: ', cost.eval(feed_dict={features: features_in, labels: labels_in}) )\n",
    "        \n",
    "print( '2_0 cost: ', cost )\n",
    "print( '2 _W: ', sess.run(_W) )\n",
    "    \n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(label_predicted, 1), tf.argmax(labels, 1))\n",
    "#print( '3 correct_prediction: ', sess.run(correct_prediction) )\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))   \n",
    "#print( '3 accuracy: ', sess.run(accuracy) )    \n",
    "\n",
    "test_data_generator = _get_next_data( './MNIST_data/t10k-images.idx3-ubyte', \n",
    "                                      './MNIST_data/t10k-labels.idx1-ubyte', 5 )\n",
    "test_features, test_labels = next( test_data_generator )\n",
    "print( accuracy.eval(feed_dict={features: test_features, labels: test_labels}) )\n",
    "    \n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  [[ 0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.1]\n",
      " [ 0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.1]\n",
      " [ 0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.1]]\n",
      "softmax result= [[ 0.11827764  0.09683754  0.09683754  0.09683754  0.09683754  0.09683754\n",
      "   0.09683754  0.09683754  0.09683754  0.10702203]\n",
      " [ 0.11827764  0.09683754  0.09683754  0.09683754  0.09683754  0.09683754\n",
      "   0.09683754  0.09683754  0.09683754  0.10702203]\n",
      " [ 0.11827764  0.09683754  0.09683754  0.09683754  0.09683754  0.09683754\n",
      "   0.09683754  0.09683754  0.09683754  0.10702203]]\n",
      "cost1_0 = [[-0.         -0.         -0.         -0.         -2.33472061 -0.         -0.\n",
      "  -0.         -0.         -0.        ]\n",
      " [-2.13472056 -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.         -0.        ]\n",
      " [-0.         -2.33472061 -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.         -0.        ]]\n",
      "cost2_0 = [ 2.33472061  2.13472056  2.33472061]\n",
      "0.226805\n",
      "2.26805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f4a18616d68>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 171, in __del__\n",
      "    self.close()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 976, in close\n",
      "    self._default_session.__exit__(None, None, None)\n",
      "  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3378, in get_controller\n",
      "    % type(default))\n",
      "AssertionError: Nesting violated for default stack of <class 'weakref'> objects\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "features_in = [[ 0.,  0.,  0., 255., 123., 0., 234., 111., 9.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0., 2., 13., 20., 24., 11., 39.,  50.,  0.,  0.],\n",
    " [ 0.,  0.,  0., 25., 12., 30., 34., 111., 93.,  0.,  0.,  0.]]\n",
    "    \n",
    "labels_in = [[ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]\n",
    "    \n",
    "    \n",
    "_W = [[ 0.2,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]\n",
    "\n",
    "_b = [ 0.2,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.1]\n",
    "\n",
    "\n",
    "logits = tf.matmul(features_in, _W) + _b\n",
    "logits = tf.Print(logits, [logits], '0 logits: ')\n",
    "\n",
    "\n",
    "label_predicted = tf.nn.softmax(logits)\n",
    "label_predicted = tf.Print(label_predicted, [label_predicted], '0 label_predicted: ')\n",
    "cost1_0 = labels_in * tf.log(label_predicted)\n",
    "cost1 = -tf.reduce_mean( cost1_0 )\n",
    "\n",
    "#################### fuck! #######################\n",
    "# cost2 = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels_in) )\n",
    "cost2_0 = tf.nn.softmax_cross_entropy_with_logits(logits, labels_in)\n",
    "cost2 = tf.reduce_mean( cost2_0 )\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "with tf.control_dependencies([tf.Print(cost1, [cost1], \"###\")]):\n",
    "    print('logits: ', sess.run(logits))\n",
    "    print('softmax result=', sess.run(label_predicted))\n",
    "    print('cost1_0 =', sess.run(cost1_0))\n",
    "    print('cost2_0 =', sess.run(cost2_0))\n",
    "    c_1 = sess.run(cost1)\n",
    "    c_2 = sess.run(cost2)\n",
    "    print(c_1)\n",
    "    print(c_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1:softmax result=\n",
      "[[ 0.11827764  0.09683754  0.09683754  0.09683754  0.09683754  0.09683754\n",
      "   0.09683754  0.09683754  0.09683754  0.10702203]\n",
      " [ 0.11827764  0.09683754  0.09683754  0.09683754  0.09683754  0.09683754\n",
      "   0.09683754  0.09683754  0.09683754  0.10702203]\n",
      " [ 0.11827764  0.09683754  0.09683754  0.09683754  0.09683754  0.09683754\n",
      "   0.09683754  0.09683754  0.09683754  0.10702203]]\n",
      "step2:cross_entropy result=\n",
      "6.80416\n",
      "Function(softmax_cross_entropy_with_logits) result=\n",
      "6.80416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f4a1aead0b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 171, in __del__\n",
      "    self.close()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 976, in close\n",
      "    self._default_session.__exit__(None, None, None)\n",
      "  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3378, in get_controller\n",
      "    % type(default))\n",
      "AssertionError: Nesting violated for default stack of <class 'weakref'> objects\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "\n",
    "#our NN's output  \n",
    "#logits=tf.constant([[1.0,2.0,3.0, 4.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0]])  \n",
    "logits = [[0.2, 0., 0., 0., 0., 0., 0., 0., 0., 0.1],\n",
    " [0.2, 0., 0., 0., 0., 0., 0., 0., 0., 0.1],\n",
    " [0.2, 0., 0., 0., 0., 0., 0., 0., 0., 0.1]]\n",
    "#step1:do softmax  \n",
    "y=tf.nn.softmax(logits)  \n",
    "#true label  \n",
    "#y_=tf.constant([[0.0,0.0,0.0,1.0],[0.0,0.0,0.0,1.0],[0.0,0.0,0.0,1.0]])  \n",
    "y_ = [[ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]\n",
    "#step2:do cross_entropy  \n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))  \n",
    "#do cross_entropy just one step  \n",
    "cross_entropy2=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits, y_))#dont forget tf.reduce_sum()!!  \n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    softmax=sess.run(y)  \n",
    "    c_e = sess.run(cross_entropy)  \n",
    "    c_e2 = sess.run(cross_entropy2)  \n",
    "    print(\"step1:softmax result=\")  \n",
    "    print(softmax)  \n",
    "    print(\"step2:cross_entropy result=\")  \n",
    "    print(c_e)  \n",
    "    print(\"Function(softmax_cross_entropy_with_logits) result=\")  \n",
    "    print(c_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
